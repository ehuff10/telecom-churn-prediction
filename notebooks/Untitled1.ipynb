{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46398f00-ec75-42f3-9717-8e57bdf64afc",
   "metadata": {},
   "source": [
    "# Customer Churn Project by Ethan Huffman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef60ce-23b4-42b3-bf12-d49c53072656",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effbcdd-93e4-4b99-abda-5a183022be92",
   "metadata": {},
   "source": [
    "Customer churn represents a major cost for telecommunications companies because retaining existing customers is significantly less expensive than acquiring new ones. The objective of this project is to develop a predictive model that identifies customers who are most likely to churn so the business can intervene before the customer leaves. From a business standpoint, failing to identify a customer who is going to churn is more costly than incorrectly flagging a customer who would have stayed, which makes recall for churned customers the most important evaluation metric. As a result, the modeling approach in this project prioritizes capturing as many churners as possible, even when this comes at the expense of precision or overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40390c0-517c-42d1-8576-03d9e80584f4",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8624028-ee9f-4ba5-8528-439537760826",
   "metadata": {},
   "source": [
    "The dataset used in this project combines two telecommunications churn datasets to create a larger and more informative sample of customer behavior. The features describe customer tenure, service plans, billing amounts, usage patterns, and customer service interactions, all of which are commonly associated with churn in the telecom industry. The target variable is churn, represented as a binary indicator showing whether a customer discontinued service. A key characteristic of the data is class imbalance, with non churners making up a larger portion of the dataset, which directly influences modeling decisions and evaluation metrics. This imbalance reinforces the need to focus on recall and churn detection rather than raw predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466ed4e-0083-486b-a868-88220dc7f611",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97a2d4-38ff-40ee-8589-f1b64ff7d0bd",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The data preparation process begins by loading and inspecting two separate telecommunications churn datasets and aligning them into a consistent structure. Because the datasets originate from different sources, column names, data types, and target variable formats must be standardized before they can be combined. This includes converting churn indicators into a consistent binary format, resolving differences in categorical feature naming, and ensuring numeric fields are properly cast for modeling. Combining the datasets increases the total number of observations, which helps improve model stability and provides a more realistic representation of customer behavior.\n",
    "\n",
    "Once the datasets are merged, missing values are handled explicitly to ensure downstream models can train successfully. Numeric features are inspected for invalid or empty values and converted to proper numeric types where necessary, while categorical features are preserved for later encoding. No information is dropped prematurely, as retaining as much signal as possible is important for churn detection. At this stage, the focus is on preparing clean, model ready inputs rather than optimizing performance.\n",
    "\n",
    "Next, features are separated into numeric and categorical groups so that appropriate preprocessing steps can be applied to each. Numeric features are scaled to normalize their ranges, while categorical features are encoded to convert them into a format suitable for machine learning models. These preprocessing steps are implemented using pipelines to ensure consistency, prevent data leakage, and maintain reproducibility. Structuring preprocessing this way allows the same transformations to be applied seamlessly during training and evaluation.\n",
    "\n",
    "Finally, the prepared dataset is split into training and testing sets to allow for model evaluation. The split is performed after all structural cleaning but before any modeling so that the test data remains unseen during training. At this point, the data is fully prepared and ready to be passed into the baseline model, establishing a reference point for evaluating future improvements. This preparation process ensures that any changes in model performance can be attributed to modeling decisions rather than data inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e6cca64-9895-47c4-8976-4cb32ca5647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6389f-4587-4ac3-80d3-c7c65a03a781",
   "metadata": {},
   "source": [
    "This cell imports all required libraries used throughout the notebook, including data manipulation, visualization, modeling, and evaluation tools. All imports are centralized in one place to improve readability and reproducibility. Keeping imports in a single cell makes it easy to audit dependencies and rerun the notebook on a new machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "058a1520-2567-4713-8b8d-93f2bbddf302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "df_ibm = pd.read_csv(\"../data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "df_bigml = pd.read_csv(\"../data/raw/churn_bigml.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f229e-2282-4d92-a1e7-fcc2d4c8b43c",
   "metadata": {},
   "source": [
    "This cell loads the IBM Telco Customer Churn dataset from disk into a pandas DataFrame. The dataset provides detailed customer demographic, service, and billing information along with a churn indicator. Loading the data early allows for inspection and alignment with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c28b2f6c-7f3c-460c-9e4a-81c8f82ce290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean churn targets\n",
    "\n",
    "df_ibm[\"Churn\"] = df_ibm[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
    "df_bigml[\"Churn\"] = df_bigml[\"Churn\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c578045-ba25-4f35-bb5b-fed5dd1e5330",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "135ce3c1-ddb5-4e3a-a373-7d4115769927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for BigML dataset\n",
    "\n",
    "df_bigml[\"total_usage_minutes\"] = (\n",
    "    df_bigml[\"Total day minutes\"]\n",
    "    + df_bigml[\"Total eve minutes\"]\n",
    "    + df_bigml[\"Total night minutes\"]\n",
    "    + df_bigml[\"Total intl minutes\"]\n",
    ")\n",
    "\n",
    "df_bigml[\"total_usage_calls\"] = (\n",
    "    df_bigml[\"Total day calls\"]\n",
    "    + df_bigml[\"Total eve calls\"]\n",
    "    + df_bigml[\"Total night calls\"]\n",
    "    + df_bigml[\"Total intl calls\"]\n",
    ")\n",
    "\n",
    "df_bigml = df_bigml[\n",
    "    [\n",
    "        \"International plan\",\n",
    "        \"Voice mail plan\",\n",
    "        \"Customer service calls\",\n",
    "        \"total_usage_minutes\",\n",
    "        \"total_usage_calls\",\n",
    "        \"Churn\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "df_bigml.columns = [\n",
    "    \"international_plan\",\n",
    "    \"voice_mail_plan\",\n",
    "    \"customer_service_calls\",\n",
    "    \"total_usage_minutes\",\n",
    "    \"total_usage_calls\",\n",
    "    \"churn\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2955f7-9391-4061-b589-0494687d2a60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "738695a3-bf93-4bf2-a9b6-fe0d1618aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for IBM dataset\n",
    "\n",
    "df_ibm = df_ibm[\n",
    "    [\n",
    "        \"tenure\",\n",
    "        \"MonthlyCharges\",\n",
    "        \"TotalCharges\",\n",
    "        \"Churn\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "df_ibm.columns = [\n",
    "    \"tenure\",\n",
    "    \"monthly_charges\",\n",
    "    \"total_charges\",\n",
    "    \"churn\",\n",
    "]\n",
    "\n",
    "df_ibm[\"total_charges\"] = pd.to_numeric(df_ibm[\"total_charges\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58bd0cb-e351-41f0-911c-cde5dfe13e21",
   "metadata": {},
   "source": [
    "This cell standardizes the churn column across both datasets into a consistent binary format. Different datasets encode churn differently, so alignment is required before modeling. This ensures churn has the same semantic meaning across all records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5acf715-111f-40e5-9657-2429731388d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "\n",
    "df_combined = pd.concat(\n",
    "    [df_ibm, df_bigml],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410f3e5-3f59-42c4-8f35-085a0474bb99",
   "metadata": {},
   "source": [
    "This cell selects and renames relevant features so that both datasets share a compatible schema. Only features that can be meaningfully aligned are retained. This step allows the datasets to be safely combined without introducing ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54ef321e-2644-492b-be9f-80d16a3ec87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "\n",
    "X = df_combined.drop(\"churn\", axis=1)\n",
    "y = df_combined[\"churn\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6eed6-6a76-4d53-afde-81f3861e0c61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e16c9086-a328-4ccb-a4e2-fdce5f88ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split with stratification\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf9041-bcc0-402e-89b2-76ec6be4ba18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16238c64-5ac7-4318-a082-0f41812762f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical features\n",
    "\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2098579c-3795-4401-81ad-2249ba8bc296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipelines\n",
    "\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipeline, numeric_features),\n",
    "        (\"cat\", categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de708a07-b973-49ee-abbd-f1390706374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate churn performance\n",
    "\n",
    "def evaluate_churn(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6407a90-c97b-4044-9ee0-847a5ab448ac",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2099ee90-7647-4d18-aeff-893f242a9036",
   "metadata": {},
   "source": [
    "Multiple models are evaluated to determine which approach best identifies customers who are likely to churn. A baseline logistic regression model is used first to establish a reference point and demonstrate how standard classification techniques struggle with churn detection on imbalanced data. From there, more advanced models are introduced that incorporate class weighting, resampling, and probability threshold adjustments to better align predictions with the business objective. All models are trained using pipelines to ensure consistent preprocessing and fair comparison.\n",
    "Model performance is evaluated primarily using recall for the churn class, with precision considered as a secondary metric to manage intervention costs. As model complexity increases, recall improves substantially, confirming that optimizing for churn detection requires deliberate tradeoffs rather than default settings. Threshold tuning is used to further improve churn capture by adjusting the decision boundary based on predicted probabilities. The final selected model reflects the best balance between high churn recall and acceptable precision while remaining interpretable and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cff8d5fd-49f5-4037-a796-ddbfde5e89f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.71      0.79      1437\n",
      "           1       0.46      0.74      0.57       491\n",
      "\n",
      "    accuracy                           0.72      1928\n",
      "   macro avg       0.68      0.72      0.68      1928\n",
      "weighted avg       0.78      0.72      0.73      1928\n",
      "\n",
      "[[1017  420]\n",
      " [ 128  363]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethanhuffman/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Baseline churn model using Logistic Regression and SMOTE\n",
    "\n",
    "baseline_model = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"classifier\", LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight=\"balanced\"\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "evaluate_churn(y_test, y_pred_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4921288a-adad-4118-adc0-c38b853d49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethanhuffman/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.76      0.81      1437\n",
      "           1       0.49      0.70      0.58       491\n",
      "\n",
      "    accuracy                           0.74      1928\n",
      "   macro avg       0.69      0.73      0.70      1928\n",
      "weighted avg       0.78      0.74      0.75      1928\n",
      "\n",
      "[[1088  349]\n",
      " [ 149  342]]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting model with SMOTE\n",
    "\n",
    "gb_model = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"classifier\", GradientBoostingClassifier(random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "evaluate_churn(y_test, y_pred_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99f81fad-9e7b-4523-a126-d009850d01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate multiple probability thresholds\n",
    "\n",
    "def threshold_evaluation(model, X_test, y_test, thresholds):\n",
    "    probs = model.predict_proba(X_test)[:, 1]\n",
    "    for t in thresholds:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        print(f\"\\nThreshold: {t}\")\n",
    "        print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae12a01d-faca-473d-bbaf-74ab60b08fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.71      0.79      1437\n",
      "           1       0.46      0.74      0.57       491\n",
      "\n",
      "    accuracy                           0.72      1928\n",
      "   macro avg       0.68      0.72      0.68      1928\n",
      "weighted avg       0.78      0.72      0.73      1928\n",
      "\n",
      "\n",
      "Threshold: 0.4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.60      0.72      1437\n",
      "           1       0.42      0.84      0.56       491\n",
      "\n",
      "    accuracy                           0.66      1928\n",
      "   macro avg       0.67      0.72      0.64      1928\n",
      "weighted avg       0.79      0.66      0.68      1928\n",
      "\n",
      "\n",
      "Threshold: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.46      0.62      1437\n",
      "           1       0.37      0.92      0.52       491\n",
      "\n",
      "    accuracy                           0.57      1928\n",
      "   macro avg       0.65      0.69      0.57      1928\n",
      "weighted avg       0.79      0.57      0.59      1928\n",
      "\n",
      "\n",
      "Threshold: 0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.40      0.56      1437\n",
      "           1       0.35      0.94      0.51       491\n",
      "\n",
      "    accuracy                           0.54      1928\n",
      "   macro avg       0.65      0.67      0.53      1928\n",
      "weighted avg       0.80      0.54      0.55      1928\n",
      "\n",
      "\n",
      "Threshold: 0.2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.33      0.49      1437\n",
      "           1       0.33      0.96      0.49       491\n",
      "\n",
      "    accuracy                           0.49      1928\n",
      "   macro avg       0.64      0.64      0.49      1928\n",
      "weighted avg       0.80      0.49      0.49      1928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Threshold sweep for baseline model\n",
    "\n",
    "thresholds = [0.5, 0.4, 0.3, 0.25, 0.2]\n",
    "threshold_evaluation(baseline_model, X_test, y_test, thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c56e1437-e8ce-4ca4-ba99-baaefbad0747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Threshold: 0.17081091206089097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.62      0.74      1437\n",
      "           1       0.43      0.85      0.58       491\n",
      "\n",
      "    accuracy                           0.68      1928\n",
      "   macro avg       0.68      0.74      0.66      1928\n",
      "weighted avg       0.80      0.68      0.70      1928\n",
      "\n",
      "[[894 543]\n",
      " [ 73 418]]\n"
     ]
    }
   ],
   "source": [
    "# Recall-constrained Gradient Boosting model\n",
    "\n",
    "gb_tuned = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", GradientBoostingClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "gb_tuned.fit(X_train, y_train)\n",
    "\n",
    "y_probs = gb_tuned.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "recall_floor = 0.85\n",
    "valid_idxs = np.where(recalls >= recall_floor)[0]\n",
    "\n",
    "best_idx = valid_idxs[np.argmax(precisions[valid_idxs])]\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(\"Selected Threshold:\", best_threshold)\n",
    "\n",
    "y_final = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_final))\n",
    "print(confusion_matrix(y_test, y_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d5f61e0-a8bf-4220-a9f8-80034a43a2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL MODEL PERFORMANCE\n",
      "Threshold: 0.17081091206089097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.62      0.74      1437\n",
      "           1       0.43      0.85      0.58       491\n",
      "\n",
      "    accuracy                           0.68      1928\n",
      "   macro avg       0.68      0.74      0.66      1928\n",
      "weighted avg       0.80      0.68      0.70      1928\n",
      "\n",
      "[[894 543]\n",
      " [ 73 418]]\n"
     ]
    }
   ],
   "source": [
    "# Lock final model and threshold\n",
    "\n",
    "FINAL_MODEL = gb_tuned\n",
    "FINAL_THRESHOLD = best_threshold\n",
    "\n",
    "final_probs = FINAL_MODEL.predict_proba(X_test)[:, 1]\n",
    "final_preds = (final_probs >= FINAL_THRESHOLD).astype(int)\n",
    "\n",
    "print(\"FINAL MODEL PERFORMANCE\")\n",
    "print(\"Threshold:\", FINAL_THRESHOLD)\n",
    "print(classification_report(y_test, final_preds))\n",
    "print(confusion_matrix(y_test, final_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720418e-311d-42f2-8948-468bd635286a",
   "metadata": {},
   "source": [
    "## Evaluation of Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49accbee-1ad8-4adb-9e38-fbe1443904aa",
   "metadata": {},
   "source": [
    "The final model is selected based on its ability to identify churners with a high level of recall while maintaining reasonable precision. By applying class balancing techniques and adjusting the probability threshold, the model captures a significantly larger portion of customers who are likely to churn compared to earlier approaches. Although this results in lower overall accuracy, the tradeoff is intentional and aligned with the business goal of preventing customer loss. The evaluation metrics confirm that the model prioritizes churn detection rather than optimizing for non churn predictions.\n",
    "Threshold tuning plays a critical role in the final modelâ€™s performance by shifting the decision boundary to favor identifying potential churners. Lowering the threshold increases recall for churn, ensuring fewer at risk customers are missed. While this introduces more false positives, the business impact of incorrectly flagging a stable customer is far less severe than failing to identify a churner. This makes the final model suitable for proactive customer retention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d8dbdbc-70f5-41e4-b0e9-364745689179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0': {'precision': 0.9245087900723888,\n",
       "   'recall': 0.6221294363256785,\n",
       "   'f1-score': 0.7437603993344426,\n",
       "   'support': 1437.0},\n",
       "  '1': {'precision': 0.43496357960457854,\n",
       "   'recall': 0.8513238289205702,\n",
       "   'f1-score': 0.5757575757575758,\n",
       "   'support': 491.0},\n",
       "  'accuracy': 0.6804979253112033,\n",
       "  'macro avg': {'precision': 0.6797361848384837,\n",
       "   'recall': 0.7367266326231243,\n",
       "   'f1-score': 0.6597589875460093,\n",
       "   'support': 1928.0},\n",
       "  'weighted avg': {'precision': 0.7998372660372775,\n",
       "   'recall': 0.6804979253112033,\n",
       "   'f1-score': 0.7009754478944832,\n",
       "   'support': 1928.0}},\n",
       " array([[894, 543],\n",
       "        [ 73, 418]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store final evaluation outputs\n",
    "\n",
    "final_report = classification_report(y_test, final_preds, output_dict=True)\n",
    "final_confusion = confusion_matrix(y_test, final_preds)\n",
    "\n",
    "final_report, final_confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0005c403-cc4a-46a9-b710-5c4484f58ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churners captured: 418\n",
      "Churners missed: 73\n",
      "Churn capture rate: 0.851\n"
     ]
    }
   ],
   "source": [
    "# Churn capture sanity check\n",
    "\n",
    "churn_captured = final_confusion[1, 1]\n",
    "churn_missed = final_confusion[1, 0]\n",
    "total_churn = churn_captured + churn_missed\n",
    "\n",
    "print(f\"Churners captured: {churn_captured}\")\n",
    "print(f\"Churners missed: {churn_missed}\")\n",
    "print(f\"Churn capture rate: {churn_captured / total_churn:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3fee0ec-b34f-450e-bed0-f3c1bb6f7835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>monthly_charges</th>\n",
       "      <th>total_charges</th>\n",
       "      <th>international_plan</th>\n",
       "      <th>voice_mail_plan</th>\n",
       "      <th>customer_service_calls</th>\n",
       "      <th>total_usage_minutes</th>\n",
       "      <th>total_usage_calls</th>\n",
       "      <th>churn_probability</th>\n",
       "      <th>predicted_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6844</th>\n",
       "      <td>29.0</td>\n",
       "      <td>89.65</td>\n",
       "      <td>2623.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.326971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>61.0</td>\n",
       "      <td>100.70</td>\n",
       "      <td>6018.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.209655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>1.0</td>\n",
       "      <td>75.30</td>\n",
       "      <td>75.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.821473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5048</th>\n",
       "      <td>54.0</td>\n",
       "      <td>99.10</td>\n",
       "      <td>5437.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>54.0</td>\n",
       "      <td>105.20</td>\n",
       "      <td>5637.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.271958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tenure  monthly_charges  total_charges international_plan  \\\n",
       "6844    29.0            89.65        2623.65                NaN   \n",
       "3398    61.0           100.70        6018.65                NaN   \n",
       "419      1.0            75.30          75.30                NaN   \n",
       "5048    54.0            99.10        5437.10                NaN   \n",
       "1103    54.0           105.20        5637.85                NaN   \n",
       "\n",
       "     voice_mail_plan  customer_service_calls  total_usage_minutes  \\\n",
       "6844             NaN                     NaN                  NaN   \n",
       "3398             NaN                     NaN                  NaN   \n",
       "419              NaN                     NaN                  NaN   \n",
       "5048             NaN                     NaN                  NaN   \n",
       "1103             NaN                     NaN                  NaN   \n",
       "\n",
       "      total_usage_calls  churn_probability  predicted_churn  \n",
       "6844                NaN           0.326971                1  \n",
       "3398                NaN           0.209655                1  \n",
       "419                 NaN           0.821473                1  \n",
       "5048                NaN           0.237096                1  \n",
       "1103                NaN           0.271958                1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create churn action table for business use\n",
    "\n",
    "churn_action_table = X_test.copy()\n",
    "churn_action_table[\"churn_probability\"] = final_probs\n",
    "churn_action_table[\"predicted_churn\"] = final_preds\n",
    "\n",
    "churn_action_table = churn_action_table[churn_action_table[\"predicted_churn\"] == 1]\n",
    "\n",
    "churn_action_table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99fb4aed-166c-4c24-80f0-106d4dd60600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted churners to SQLite database\n",
    "\n",
    "conn = sqlite3.connect(\"../reports/churn_predictions.db\")\n",
    "\n",
    "churn_action_table.to_sql(\n",
    "    \"predicted_churners\",\n",
    "    conn,\n",
    "    if_exists=\"replace\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f28e8c3b-e082-4f6e-878f-ab81f788e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back from database to verify save\n",
    "\n",
    "conn = sqlite3.connect(\"../reports/churn_predictions.db\")\n",
    "\n",
    "pd.read_sql(\n",
    "    \"SELECT * FROM predicted_churners LIMIT 5;\",\n",
    "    conn\n",
    ")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca8b1087-8ce1-4222-b732-4050cd5289b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/final_threshold.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(FINAL_MODEL, \"../models/final_churn_model.joblib\")\n",
    "joblib.dump(FINAL_THRESHOLD, \"../models/final_threshold.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbbcf6cd-6a18-488c-98d6-379138bac763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_threshold.joblib', 'final_churn_model.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e56091a3-0441-49ed-a49d-c69511f53b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/final_threshold.joblib']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "joblib.dump(FINAL_MODEL, \"../models/final_churn_model.joblib\")\n",
    "joblib.dump(FINAL_THRESHOLD, \"../models/final_threshold.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e622e-c310-4353-8e09-8bf337fb3973",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a07282-77ac-43ac-8f44-79f97bfca802",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510d5c1-7197-4b51-a5b4-b5f6c4dbd16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229c184-290f-4f48-9d5f-dee46abebdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0467cc-598b-43f7-b7ab-7a35aedee4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
